{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33255008-bd03-4ff9-a2fb-c75c24c65101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0.68023941 0.45007456 0.83983248 0.99371238 0.42765172 0.83419885\n",
      " 0.53664422 0.19829556 0.14298862 0.36819451] 10 0.9937123793337669 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BernoulliBandit :\n",
    "    def __init__(self, K) :\n",
    "        self.probs = np.random.uniform(size=K)\n",
    "        self.K = K\n",
    "        self.best_idx = np.argmax(self.probs)\n",
    "        self.best_prob = self.probs[self.best_idx]\n",
    "        \n",
    "    def step(self, k:int) :\n",
    "        if np.random.rand() < self.probs[k] :\n",
    "            return 1\n",
    "        return 0\n",
    "    def status(self) :\n",
    "        print(self.probs, self.K, self.best_prob, self.best_idx)\n",
    "\n",
    "bb = BernoulliBandit(10)\n",
    "reward = bb.step(int(2))\n",
    "print(reward)\n",
    "bb.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0caee0e1-dbec-4b17-b2f5-0dce59752348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: [0, 2, 0, 2, 1, 1, 1, 2, 1, 0]\n",
      "regrets: [0.15951352703764154, 0.5660606615284806, 0.8507237622377661, 0.5436378152795822, 0.7954168219216354, 0.0, 0.5436378152795822, 0.45706816404935324, 0.0, 0.7954168219216354]\n"
     ]
    }
   ],
   "source": [
    "class Solver :\n",
    "    def __init__(self, bandit) :\n",
    "        self.bandit = bandit\n",
    "        self.counter = [0 for i in range(bandit.K)]\n",
    "        self.regrets = [] #[0 for i in range(bandit.K)] 是懊悔记录，不是每个臂的懊悔，别跟reward混淆\n",
    "    def policy(self) :\n",
    "        # choose one bandit and return\n",
    "        return np.random.randint(0, self.bandit.K)\n",
    "\n",
    "    def update_regret(self, k) :\n",
    "        regret = self.bandit.best_prob - self.bandit.probs[k]\n",
    "        self.regrets.append(regret)\n",
    "    \n",
    "    def run(self, max_step) :\n",
    "        for i in range(max_step) :\n",
    "            k = self.policy()\n",
    "            self.counter[k] += 1\n",
    "            self.update_regret(k)\n",
    "            \n",
    "    def status(self) :\n",
    "        print(\"counter:\", self.counter)\n",
    "        print(\"regrets:\", self.regrets)\n",
    "s = Solver(bb)\n",
    "s.run(10)\n",
    "s.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae437cdc-5850-4bc0-94b2-433e7911a8f1",
   "metadata": {},
   "source": [
    "MDP：马尔科夫决策过程  \n",
    "MP：马尔科夫过程  \n",
    "MP的策略$$\\pi(a|s)$$是已经确定了的。但MDP是不确定的、会变化的。强化学习就是要不断调整迭代$$\\pi(a|s)$$，最终得到一个最优的$$\\pi(a|s)$$，所以研究的主要是MDP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd715f1-2d35-40d2-8214-7bd26eae3ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
